# TLG8 ANNで正答率95%を目指す学習戦略（サイド情報推定特化版）

## 1. 前提と課題整理
- 目的は「各圧縮フェーズで最適なサイド情報を事前推定し、虱潰し探索を省くこと」であり、サイド情報そのものを入力特徴として利用しない。
- 現行実装（多クラスロジスティック回帰、画素正規化のみ）では Top-1 精度が 10〜50% 台に留まり、特に候補数の多いフィルタ/リオーダ段階で学習不足が顕著。
- 収集データは `tools/train_tlg8_ann.py` で扱う JSONL 形式で用意済みだが、テスト画像のみでは枚数・分布ともに不足。

## 2. 95% 精度に向けた方針
1. **データ拡充と分布制御**
   - 既存フックで得られる JSONL を継続利用しつつ、社内に保有する数千枚の著作権付き画像をローカル環境でエンコードし、学習データを桁増しに拡充する。
   - クラス不均衡対策として、フェーズ別・設定ID別に出現回数を把握し、希少クラスを優先採取するストラタムサンプリングを導入。
   - 画素情報のみを入力とする制約の下で、ブロック内のトリミング/回転/ミラーリング、輝度スケーリング、ノイズ注入などを適用し、ローカルでデータ拡張。

2. **特徴量設計（サイド情報非依存）**
   - RGB→YCoCg/YCbCr への色空間変換、ブロック内の輝度・色差統計量（平均・分散・歪度・尖度）、勾配ヒストグラム、テクスチャ指標など、画素から導出可能な数値のみを特徴量として抽出。
   - 8×8 DCT や Haar 小波などの周波数特徴、方向性フィルタ応答、局所自己相関などを追加し、圧縮設定と相関の高い情報を多面的に表現。
   - フェーズごとの特性に合わせ、共有の畳み込みバックボーン＋フェーズ別ヘッド構成のマルチタスクモデルで共通特徴を学習させる。

3. **モデルアーキテクチャと学習テクニック**
   - 軽量 CNN（例: 3〜4 層 Depthwise-Separable Conv + Global Average Pooling）をバックボーンに採用し、各フェーズの出力ヘッドは小規模 MLP で構成。
   - ラベルスムージングとクラス重み、Focal Loss を組み合わせて希少クラスの学習を安定化。
   - 95% Top-1 達成にはハイパーパラメータ探索（Optuna 等）で学習率スケジュール、バッチサイズ、データ拡張強度を最適化。
   - 学習後は Teacher-Student 蒸留で軽量化し、C++ 組込み時は Student モデルを使用。

4. **評価指標とフィードバックループ**
   - 学習時はデータを訓練 80% / 検証 10% / テスト 10% に分割し、フェーズ別 Top-1 / Top-2 精度と混同行列をモニタリング。
   - 検証セットで 95% を下回るフェーズについては、該当クラスのデータを追加採取し再学習。
   - 推論時に出力確信度（Softmax 最大値）を併記し、閾値以下のブロックのみ従来探索へフォールバックして品質低下を防止。

## 3. ローカル環境でのデータ収集と学習手順
1. **データ収集**
   - 著作権付き画像を含む任意の画像集合をローカルマシンに配置。
   - `tlgconv` に `--training-dump=<path>` `--training-tag=<tag>` を付与して TLg8 エンコードを実行し、ブロック単位の JSONL を収集。
   - 大規模画像集合では、並列処理用のシェルスクリプトや GNU Parallel を用いてバッチ処理。

2. **データ管理**
   - 収集した JSONL をフェーズ別に分割・集約するための Python スクリプト（例: `tools/prepare_tlg8_dataset.py` を後日実装）で前処理。
   - 訓練/検証/テストに分割し、メタデータ（画像名、日時など）を外部に漏らさないようローカルのみで管理。

3. **学習実行**
   - ローカル環境に PyTorch、numpy、scikit-learn、Optuna 等をインストール。
   - 学習用スクリプト（例: `tools/train_tlg8_ann.py` の拡張版）に以下のオプションを追加して使用:
     - `--model cnn` : CNN バックボーンを選択
     - `--features pixel,color,frequency,texture` : 画素由来特徴を有効化
     - `--loss focal` : Focal Loss を利用
     - `--hpo` : Optuna による自動ハイパーパラメータ探索を有効化
   - 学習ログにはフェーズ別 Top-1/Top-2/Top-5 精度、混同行列、収束曲線を出力し、95% 達成を確認。

4. **モデル評価・配備**
   - テストセットで 95% を満たしたら、モデルを ONNX 形式にエクスポートし、量子化を実施。
   - C++ 組込み用に `src/tlg8_ann_infer.cpp`（仮）を作成し、ONNX Runtime や独自推論コードでモデルを呼び出す。
   - 推論ロジックでは、ANN が返した Top-2 候補に限定して探索を行い、閾値以下は従来探索へフォールバック。
## 4. リスクと対策
- **データ漏洩リスク**: 著作権付き画像はローカルから持ち出さず、生成したモデルのみを共有。学習パイプラインのログから原画像を逆推定できないよう留意。
- **モデル肥大化**: Depthwise Conv、蒸留、INT8 量子化でパラメータ数と推論遅延を抑制。
- **精度劣化時の品質悪化**: 信頼度閾値を設け、ANN が不確実なブロックでは虱潰し探索にフォールバック。
- **学習コスト**: データ数が増えるほど学習時間が延びるため、HPO 期間中は GPU を活用し、学習ジョブをバッチ化して夜間実行。

## 5. 今後のアクション
1. `tools/train_tlg8_ann.py` を CNN / HPO 対応へ拡張し、特徴量前処理コードを追加。
2. データ前処理・分割スクリプトを `tools/` 配下に新設し、ローカル画像群からの学習を再現可能にする。
3. 推論統合用の C++ 実装案（API、フォールバック戦略、メモリ管理）を文書化。
4. 95% 精度を達成したモデルの推論時間・圧縮率改善効果を測定し、既存パイプラインと比較評価。
